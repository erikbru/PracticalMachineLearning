---
title: Practical Machine Learning; Predicting the manner in which Weight Lifting Exercises were done.
author: "Erik Bruin"
output: html_document
---

## Executive Summary

This project was carried out as the final assignment of the Coursera course "Practical Machine Learning". The purpose is to predict the manner in which weight lifting exercises were done. Key findings were:

* Of the original data set with the respons variable "classe" with 159 possible predictors, 52 predictors/features were selected to build the prediction model on.
* I managed to build a Random Forest model with 99.3% accuracy
* The small dataset with new observations (20 observations) provided by Coursera was predicted with 100% accuracy.


The online versions of this document can be found at [GitHub](https://github.com/erikbru/practicalmachinelearning), and a fully rendered html document can be found at [GitHub Pages](https://erikbru.github.io/practicalmachinelearning/).

## Introduction

This project was carried out as the final assignment of the coursera course "Practical Machine Learning". The purpose is to predict the manner in which weight lifting exercises were done. The website with additional information says: Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).

In order to determine in which fashion a particular exercise was performed, 4 technical devices (accelerometers) were used which record so called "quantified self movements". The goal is to predict the correct class of execution based on data from these accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(cache=TRUE) #Set-up Knitr in such a way that it only reruns chunks if changes are detected
```

```{r ref.label="libraries", echo=FALSE, message=FALSE}
#load libraries
#the named code chunk "libraries", which is in the appendix, is run here with option ref.label=.
#chunk option echo=FALSE ensures that the code is not printed at this place in the html
# I am using this option a lot, so please look at the code in the appendix
```


## Data exploration and feature selection

Note to the "technical" reader: As coming across pieces of code is likely to distract business people from the results of the analysis, I chose to put all code chunks in an Appendix. However, the these chunks of code are run at the appropriate places in this document (using reference labels). In addition, as cross validation and expected out of sample errors are specifically mentioned as review criteria, I have highlighted these answers in bold.

```{r ref.label="loadData", echo=FALSE, results='hide'}
#downloading the data and reading into R. Results are hidden (trying URL etc)
```

The data set consists of 160 variables, with a training set of 19,622 observations and 160 variables, and a very small testing set of only 20 observations (and also 160 variables). The instructions for this project are: The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with. 

Consequently, I first want to get a feel for the respons variable (classe). As you can see below, there are indeed 5 different outcomes (A-E) in the training set. Class A (exactly according to the specifications) has most observations, but the other classes also each have a substantial number of observations.

```{r strClasse, echo=FALSE}
qplot(training$classe, main="Figure 1: Distribution of Observations in the Training Set", xlab="Observed Classe", ylab="Number of Observations") + scale_y_continuous(breaks = scales::pretty_breaks(n = 10))
```


```{r runFeature, ref.label="featureSelection", echo=FALSE}
#67 columns with NAs
```

My first step in selecting usable features for prediction is to investigate which of the 160 variables contain missing values (stored as NA (Not Available)). The result is that 67 variables contain NA values.  Actually, all these variables have 19216 NAs out of a total of 19622 observations. As almost 98% of values being unavailable means that these variables cannot possibly be good predictors, I am excluding them from the analysis. This leaves me with 93 variables.

The website indicates that some summary statistics are also included in the data ("For the Euler angles of each of the four sensors we calculated  eight  features:  mean,  variance,standard deviation, max, min, amplitude, kurtosis and skewness"). It seems best to exclude columns that contain summary statistics, and work with raw data only. Out of the 93 variables that I had left after excluding the columns with mostly NAs, an additional 33 variables contain summary statistics. Also excluding these leaves me with 60 variables. .

Before I move on to building a prediction model, I now want to inspect the remaining 60 variables. It turns out that the 1st column just duplicates the row index, and column 2-6 contain information on the user and the timestamps of the measurements. As we only have to predict if an exercise was performed correctly (or which error was made), it does not seem relevant to me who the user was (it all depends on the measurements) or when the activity was executed. Therefore, I can also get rid of column 1-7. The remaining columns are all numeric, and are equipment measurements.

This leaves me with a dataframe of 53 variables. Althought I believe that this data is clean and ready to use for model building, I want to check if there are somehow still variables left with Nero Zero Variance (and hence not useful for prediction). However, running a Near Zero Variance analysis on these 53 remaining variables shows that are 53 variables are relevant. The results of the data cleaning and feature selection step is that I now have a training dataframe with 53 columns (the response variable "classe", and 52 predictors) and 19,622 observations. This will be the dataframe that I will use to build my prediction model on.

## Splitting the dataset into a training and test set

The testing set provided is only a very small set that we are going to use for the last quiz in this course. It cannot be used for model building. In order to build a machine learning algorithm, I am going to split the training set into a train set (60% of observations) and a test set (40%). The distribution of observations is shown in the table below. Besides the 60/40 split on the total number of observations, the class distribution is also preserved (meaning that there is a 60/40 split on all classes as well). 

```{r runSplitSet, ref.label="splitset", echo=FALSE}
#splitting trainingClean into a train and a test set
```

```{r runTable, ref.label="tableTrainTest", echo=FALSE}
#creating a Table with numbers of observations in train and test
```


## Trying different models

In the discussion forum, I read that we have to try different models until we have a model with at least 99% accuracy (apparently that is possible with this data set).

### Model 1: Building a Decision Tree (CART model)

Since the response variable is categorical (not binary, but with 5 different outcomes), the most logical thing to do is to build a single classification tree first and see what the resulting accuracy is. With decision trees, there is no need to standardize the (numerical) data (No preProcess=c('center', 'scale')). The resampling method that I am using is **cross validation with k=5 folds** (see model specification code in the Appendix).

The complexity parameter (cp) is used to control the size of the decision tree and to select the optimal tree size. If the cost of adding another variable to the decision tree from the current node is above the value of cp, then tree building does not continue. First, I did not specify the tuneLenght. This led to Caret only checking 3 values for cp, and when I used the model to compose the confusion matrix for the test set, I noticed that D did not get predicted at all. Therefore, I think this model was really underfitting (pruned too much). I then increased the tuneLenght to let caret check 5 possible cp's. The model is now a bit more complex but it seems much more reasonable (the more granular cp search led to a smaller cp, and hence a larger tree).

```{r runCART, ref.label="CARTmodel", echo=FALSE}
#running rPart model
```

In order to determine the accuracy on the test set and the expected out of sample error, I first have to make predictions on the test set and compose the confusion matrix.

```{r runPredictCART, ref.label="predictCART", echo=FALSE}
#predictions on test set, and confusion matrix on test set
```

As you can see, the accuracy on the test set is only 54% (meaning that the **expected out of sample error is about 46% (1-accuracy)**). This is really not good enough, and therefore, I have to use a more complex algorithm.

### Model 2: Random Forest

When talking about more sophisticated models for classification based on trees, I could try Random Forest (Bagging approach), or for instance a GBM model (Boosting approach). I will first try to see if I can achieve the required 99% accuracy with Random Forest.

Fisrt of all, I am changing the default resampling method in caret from bootstrap resampling (25 reps) to **cross validation**. Usually, k=5 or k=10 folds is sufficient, and in order to make the algorithm faster **I am using k=5 folds** instead of the default of 10. Altogether, this decreases the resampling from the default 25 reps to only 5.

The main tuning parameters in Random Forest are:  
* mtry: Number of variables randomly sampled as candidates at each split.  
* ntree: Number of trees to grow

The default value of ntree is 500. This is generally enough to get a sufficient amount of votes for each observation, and I have no reason to believe this would be different for this dataset. However, mtry is more important with regards to the model accuracy. The default is that caret tried mrty=2 (the minimum), mtry=52(all predictors), and a value in between, mtry=27. However, I believed that it should be possible to find a better value for mtry as the 'gaps' between the default mtry options are huge. By trying tuneLenght=10, I achieve that 10 different mtry's are checked with incremental steps of 5. Although accuracy values for all mtry values is high, a value 'in "between" is selected indeed (mtry=13), which makes this model slightly better than the one found with the default tuneLenght.
```{r runRF, ref.label="rfmodel1", echo=FALSE}
#running the RF model
```

```{r runConfTrain, ref.label="confTRAIN", echo=FALSE}
#composing the confusion matrix for the train set
```
In the results above, you can see that the accuracy on the train set is 100% (the confusion matrix maps predicted results with observed classes). However, in order to see how well the model generalizes to new data, the confusion matrix on the test set must be composed (below).
```{r runConfTest, ref.label="confTEST", echo=FALSE}
#composing the confusion matrix for the test set
```

As I have managed to get an accuracy on the test set of more than 99% (99.3%, meaning that **the expected out of sample error is just 0.7% (1-accuracy)**), there is no need to also try a GBM model.

Although the accuracy of the resulting model is very good, it is still interesting to check which variables are the important ones in this model. The 10 most important ones are show in the figure below.

```{r varImp, echo=FALSE}
varImpPlot(modelRF$finalModel, n.var=10, main=" Top 10 of most important variables")
```

GINI measures the average gain of purity by splits of a given variable. The Gini index of a pure node (consisting of 1 class) is zero, and the maximum Gini index is reached when all classes in a node have equal probability (impure). So purity increases when the Gini decreases, and therefore the variable with the highest mean decrease in Gini is the most important one (in this case roll_belt). Below roll_belt, you can see that there is a group of 6 variables that are also substantially more important than the remaining 45 predictors, as there is a gap in the MeanDecreaseGini between row 7 and 8.

## Predictions on the testing set

The last Quiz in the Practical Machine Learning course consists of predicting the classe of the 20 observations that were provided in the testing set, using your own prediction algorithm. As I have managed to get a 99.3% accuracy on the test set (not the testing set, but on the 40% 'test' cut of the training set), predictions on this (small) set of new data provided should be good.
```{r runQuiz4, ref.label="testingQuiz4", echo=FALSE}
#running prediction on testing set provided
```

I was very pleased to see that after submitting these predictions on Coursera, I had a 100% correct score!

## Appendix

The code chunks printed in this appendix are actually run at the appropriate places in this analysis (using reference labels).

Loading libraries.
```{r libraries, echo=TRUE, eval=FALSE}
library(ggplot2)
library(caret)
library(rpart)
library(rattle)
library(randomForest)
```

Downloading the files into the current working directory and load the data into R.
```{r loadData, echo=TRUE, eval=FALSE}
fileURL='https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv'
        download.file(fileURL, 'plm-training.csv')
        training= read.csv('plm-training.csv')
fileURL1='https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv'
        download.file(fileURL1, 'plm-testing.csv')
        testing=read.csv('plm-testing.csv')
```

Feature selection. Removing variables that I do not want to use in model building.
```{r featureSelection, echo=TRUE, eval=FALSE}
#Check columns with NAs
sumNA=sapply(training, function(x) {sum(is.na(x))})
sumNApos=sumNA[sumNA>0] #vector of lenghth 67, all values 19216
#remove NA columns
indexColsNA=which(sumNA>0)
trainingNoNA=training[,-indexColsNA]
#remove remaining columns with summary statistics
indexSummaryCols=grep("^kurtosis|^skewness|^min|^max|^amplitude|^var|^avg|^stddev", names(trainingNoNA))
trainingClean=trainingNoNA[,-indexSummaryCols] #indexSummaryCols is vector of length 33
#remove first 7 columns
trainingClean=trainingClean[,-c(1:7)]
#check if any of remaining columns have Near Zero Variance
nzvClean <- nearZeroVar(trainingClean,saveMetrics=TRUE)
countNZV=nzvClean[nzvClean$nzv==TRUE,]
numNearZeroVar=nrow(countNZV) #result=0
#repeat column selection on testing set
testingNoNa=testing[, -indexColsNA]
testingClean=testingNoNa[, -indexSummaryCols]
testingClean=testingClean[,-c(1:7)]
```

Split training into train and test sets
```{r splitset, echo=TRUE, eval=FALSE}
set.seed(74845307)
inTrain=createDataPartition(y=trainingClean$classe, p=0.6, list=FALSE)
train=trainingClean[inTrain,]
test=trainingClean[-inTrain,]
#Clean up Global Environment before modeling, only keeping train, test, and testingClean
rm(fileURL); rm(fileURL1); rm(training); rm(testing); rm(sumNA); rm(sumNApos); rm(indexColsNA); rm(trainingNoNA); rm(indexSummaryCols); rm(nzvClean); rm(countNZV); rm(numNearZeroVar); rm(testingNoNa); rm(inTrain); rm(trainingClean)
```

Make table with numbers in train and test set
```{r tableTrainTest, echo=TRUE, eval=FALSE}
tab1=rbind(table(train$classe), table(test$classe))
tab1=rbind(tab1, (colSums(tab1)))
tab1=cbind(tab1, (rowSums(tab1)))
tab1=cbind(tab1, round(c(prop.table(tab1[1:2,6])*100, sum(prop.table(tab1[1:2,6])*100))))
rownames(tab1)=c("Training set", "Test set", "Total")
colnames(tab1)=c("A", "B", "C", "D", "E", "Total", "Percentage")
kable(tab1)
```

Running CART model and printing Tree
```{r CARTmodel, echo=TRUE, eval=FALSE}
set.seed(23781367)
modelCART=train(classe~., data=train, method='rpart', trControl=trainControl(method="cv", number=5), tuneLength=5)
modelCART
fancyRpartPlot(modelCART$finalModel)
```

Confusion matrix test set CART model
```{r predictCART, echo=TRUE, eval=FALSE}
predCART=predict(modelCART, test)
confCART=confusionMatrix(predCART, test$classe)
confCART
```

Running the Random Forest Model.
```{r rfmodel1, echo=TRUE, eval=FALSE}
set.seed(67569872)
modelRF=train(classe~., data=train, method='rf', trControl=trainControl(method="cv", number=5), tuneLength=10)
modelRF
```

Confusion matrix for the train set.
```{r confTRAIN, echo=TRUE, eval=FALSE}
predRFtrain=predict(modelRF, train)
confRFtrain=confusionMatrix(predRFtrain, train$classe)
confRFtrain
```
Confusion matrix for the test set
```{r confTEST, echo=TRUE, eval=FALSE}
predRFtest=predict(modelRF, test)
confRFtest=confusionMatrix(predRFtest, test$classe)
confRFtest
```

Use RF model to predict classes on testing data provided
```{r testingQuiz4, echo=TRUE, eval=FALSE}
predQuiz4=predict(modelRF, testingClean)
predQuiz4
```
